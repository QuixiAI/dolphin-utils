lm_head,[129280,7168],BF16
model.embed_tokens,[129280,7168],BF16
model.layers.61.eh_proj,[7168,14336],BF16
model.layers.61.embed_tokens,[129280,7168],BF16
model.layers.61.enorm,[7168],BF16
model.layers.61.hnorm,[7168],BF16
model.layers.61.shared_head.head,[129280,7168],BF16
model.layers.61.shared_head.norm,[7168],BF16
model.layers.[0-2].mlp.down_proj,[7168,18432],F8_E4M3
model.layers.[0-2].mlp.down_proj.weight_scale_inv,[56,144],F32
model.layers.[0-2].mlp.gate_proj,[18432,7168],F8_E4M3
model.layers.[0-2].mlp.gate_proj.weight_scale_inv,[144,56],F32
model.layers.[0-2].mlp.up_proj,[18432,7168],F8_E4M3
model.layers.[0-2].mlp.up_proj.weight_scale_inv,[144,56],F32
model.layers.[0-61].input_layernorm,[7168],BF16
model.layers.[0-61].post_attention_layernorm,[7168],BF16
model.layers.[0-61].self_attn.kv_a_layernorm,[512],BF16
model.layers.[0-61].self_attn.kv_a_proj_with_mqa,[576,7168],F8_E4M3
model.layers.[0-61].self_attn.kv_a_proj_with_mqa.weight_scale_inv,[5,56],F32
model.layers.[0-61].self_attn.kv_b_proj,[32768,512],F8_E4M3
model.layers.[0-61].self_attn.kv_b_proj.weight_scale_inv,[256,4],F32
model.layers.[0-61].self_attn.o_proj,[7168,16384],F8_E4M3
model.layers.[0-61].self_attn.o_proj.weight_scale_inv,[56,128],F32
model.layers.[0-61].self_attn.q_a_layernorm,[1536],BF16
model.layers.[0-61].self_attn.q_a_proj,[1536,7168],F8_E4M3
model.layers.[0-61].self_attn.q_a_proj.weight_scale_inv,[12,56],F32
model.layers.[0-61].self_attn.q_b_proj,[24576,1536],F8_E4M3
model.layers.[0-61].self_attn.q_b_proj.weight_scale_inv,[192,12],F32
model.layers.[3-61].mlp.experts.[0-255].down_proj,[7168,2048],F8_E4M3
model.layers.[3-61].mlp.experts.[0-255].down_proj.weight_scale_inv,[56,16],F32
model.layers.[3-61].mlp.experts.[0-255].gate_proj,[2048,7168],F8_E4M3
model.layers.[3-61].mlp.experts.[0-255].gate_proj.weight_scale_inv,[16,56],F32
model.layers.[3-61].mlp.experts.[0-255].up_proj,[2048,7168],F8_E4M3
model.layers.[3-61].mlp.experts.[0-255].up_proj.weight_scale_inv,[16,56],F32
model.layers.[3-61].mlp.gate,[256,7168],BF16
model.layers.[3-61].mlp.gate.e_score_correction_bias,[256],F32
model.layers.[3-61].mlp.shared_experts.down_proj,[7168,2048],F8_E4M3
model.layers.[3-61].mlp.shared_experts.down_proj.weight_scale_inv,[56,16],F32
model.layers.[3-61].mlp.shared_experts.gate_proj,[2048,7168],F8_E4M3
model.layers.[3-61].mlp.shared_experts.gate_proj.weight_scale_inv,[16,56],F32
model.layers.[3-61].mlp.shared_experts.up_proj,[2048,7168],F8_E4M3
model.layers.[3-61].mlp.shared_experts.up_proj.weight_scale_inv,[16,56],F32
model.norm,[7168],BF16
