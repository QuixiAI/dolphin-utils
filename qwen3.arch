Qwen/Qwen3-0.6B:
lm_head,[151936,1024],BF16
model.embed_tokens,[151936,1024],BF16
model.layers.[0-27].input_layernorm,[1024],BF16
model.layers.[0-27].mlp.down_proj,[1024,3072],BF16
model.layers.[0-27].mlp.gate_proj,[3072,1024],BF16
model.layers.[0-27].mlp.up_proj,[3072,1024],BF16
model.layers.[0-27].post_attention_layernorm,[1024],BF16
model.layers.[0-27].self_attn.k_norm,[128],BF16
model.layers.[0-27].self_attn.k_proj,[1024,1024],BF16
model.layers.[0-27].self_attn.o_proj,[1024,2048],BF16
model.layers.[0-27].self_attn.q_norm,[128],BF16
model.layers.[0-27].self_attn.q_proj,[2048,1024],BF16
model.layers.[0-27].self_attn.v_proj,[1024,1024],BF16
model.norm,[1024],BF16

Qwen/Qwen3-1.7B:
lm_head,[151936,2048],BF16
model.embed_tokens,[151936,2048],BF16
model.layers.[0-27].input_layernorm,[2048],BF16
model.layers.[0-27].mlp.down_proj,[2048,6144],BF16
model.layers.[0-27].mlp.gate_proj,[6144,2048],BF16
model.layers.[0-27].mlp.up_proj,[6144,2048],BF16
model.layers.[0-27].post_attention_layernorm,[2048],BF16
model.layers.[0-27].self_attn.k_norm,[128],BF16
model.layers.[0-27].self_attn.k_proj,[1024,2048],BF16
model.layers.[0-27].self_attn.o_proj,[2048,2048],BF16
model.layers.[0-27].self_attn.q_norm,[128],BF16
model.layers.[0-27].self_attn.q_proj,[2048,2048],BF16
model.layers.[0-27].self_attn.v_proj,[1024,2048],BF16
model.norm,[2048],BF16

Qwen/Qwen3-4B:
model.embed_tokens,[151936,2560],BF16
model.layers.[0-35].input_layernorm,[2560],BF16
model.layers.[0-35].mlp.down_proj,[2560,9728],BF16
model.layers.[0-35].mlp.gate_proj,[9728,2560],BF16
model.layers.[0-35].mlp.up_proj,[9728,2560],BF16
model.layers.[0-35].post_attention_layernorm,[2560],BF16
model.layers.[0-35].self_attn.k_norm,[128],BF16
model.layers.[0-35].self_attn.k_proj,[1024,2560],BF16
model.layers.[0-35].self_attn.o_proj,[2560,4096],BF16
model.layers.[0-35].self_attn.q_norm,[128],BF16
model.layers.[0-35].self_attn.q_proj,[4096,2560],BF16
model.layers.[0-35].self_attn.v_proj,[1024,2560],BF16
model.norm,[2560],BF16

Qwen/Qwen3-8B:
lm_head,[151936,4096],BF16
model.embed_tokens,[151936,4096],BF16
model.layers.[0-35].input_layernorm,[4096],BF16
model.layers.[0-35].mlp.down_proj,[4096,12288],BF16
model.layers.[0-35].mlp.gate_proj,[12288,4096],BF16
model.layers.[0-35].mlp.up_proj,[12288,4096],BF16
model.layers.[0-35].post_attention_layernorm,[4096],BF16
model.layers.[0-35].self_attn.k_norm,[128],BF16
model.layers.[0-35].self_attn.k_proj,[1024,4096],BF16
model.layers.[0-35].self_attn.o_proj,[4096,4096],BF16
model.layers.[0-35].self_attn.q_norm,[128],BF16
model.layers.[0-35].self_attn.q_proj,[4096,4096],BF16
model.layers.[0-35].self_attn.v_proj,[1024,4096],BF16
model.norm,[4096],BF16

Qwen/Qwen3-14B:
lm_head,[151936,5120],BF16
model.embed_tokens,[151936,5120],BF16
model.layers.[0-39].input_layernorm,[5120],BF16
model.layers.[0-39].mlp.down_proj,[5120,17408],BF16
model.layers.[0-39].mlp.gate_proj,[17408,5120],BF16
model.layers.[0-39].mlp.up_proj,[17408,5120],BF16
model.layers.[0-39].post_attention_layernorm,[5120],BF16
model.layers.[0-39].self_attn.k_norm,[128],BF16
model.layers.[0-39].self_attn.k_proj,[1024,5120],BF16
model.layers.[0-39].self_attn.o_proj,[5120,5120],BF16
model.layers.[0-39].self_attn.q_norm,[128],BF16
model.layers.[0-39].self_attn.q_proj,[5120,5120],BF16
model.layers.[0-39].self_attn.v_proj,[1024,5120],BF16
model.norm,[5120],BF16

Qwen/Qwen3-32B:
lm_head,[151936,5120],BF16
model.embed_tokens,[151936,5120],BF16
model.layers.[0-63].input_layernorm,[5120],BF16
model.layers.[0-63].mlp.down_proj,[5120,25600],BF16
model.layers.[0-63].mlp.gate_proj,[25600,5120],BF16
model.layers.[0-63].mlp.up_proj,[25600,5120],BF16
model.layers.[0-63].post_attention_layernorm,[5120],BF16
model.layers.[0-63].self_attn.k_norm,[128],BF16
model.layers.[0-63].self_attn.k_proj,[1024,5120],BF16
model.layers.[0-63].self_attn.o_proj,[5120,8192],BF16
model.layers.[0-63].self_attn.q_norm,[128],BF16
model.layers.[0-63].self_attn.q_proj,[8192,5120],BF16
model.layers.[0-63].self_attn.v_proj,[1024,5120],BF16
model.norm,[5120],BF16

Qwen/Qwen3-30B-A3B:
lm_head,[151936,2048],BF16
model.embed_tokens,[151936,2048],BF16
model.layers.[0-47].input_layernorm,[2048],BF16
model.layers.[0-47].mlp.experts.[0-127].down_proj,[2048,768],BF16
model.layers.[0-47].mlp.experts.[0-127].gate_proj,[768,2048],BF16
model.layers.[0-47].mlp.experts.[0-127].up_proj,[768,2048],BF16
model.layers.[0-47].mlp.gate,[128,2048],BF16
model.layers.[0-47].post_attention_layernorm,[2048],BF16
model.layers.[0-47].self_attn.k_norm,[128],BF16
model.layers.[0-47].self_attn.k_proj,[512,2048],BF16
model.layers.[0-47].self_attn.o_proj,[2048,4096],BF16
model.layers.[0-47].self_attn.q_norm,[128],BF16
model.layers.[0-47].self_attn.q_proj,[4096,2048],BF16
model.layers.[0-47].self_attn.v_proj,[512,2048],BF16
model.norm,[2048],BF16

Qwen/Qwen3-235B-A22B
lm_head,[151936,4096],BF16
model.embed_tokens,[151936,4096],BF16
model.layers.[0-93].input_layernorm,[4096],BF16
model.layers.[0-93].mlp.experts.[0-127].down_proj,[4096,1536],BF16
model.layers.[0-93].mlp.experts.[0-127].gate_proj,[1536,4096],BF16
model.layers.[0-93].mlp.experts.[0-127].up_proj,[1536,4096],BF16
model.layers.[0-93].mlp.gate,[128,4096],BF16
model.layers.[0-93].post_attention_layernorm,[4096],BF16
model.layers.[0-93].self_attn.k_norm,[128],BF16
model.layers.[0-93].self_attn.k_proj,[512,4096],BF16
model.layers.[0-93].self_attn.o_proj,[4096,8192],BF16
model.layers.[0-93].self_attn.q_norm,[128],BF16
model.layers.[0-93].self_attn.q_proj,[8192,4096],BF16
model.layers.[0-93].self_attn.v_proj,[512,4096],BF16
model.norm,[4096],BF16

Qwen/Qwen2.5-72B-Instruct
lm_head,[152064,8192],BF16
model.embed_tokens,[152064,8192],BF16
model.layers.[0-79].input_layernorm,[8192],BF16
model.layers.[0-79].mlp.down_proj,[8192,29568],BF16
model.layers.[0-79].mlp.gate_proj,[29568,8192],BF16
model.layers.[0-79].mlp.up_proj,[29568,8192],BF16
model.layers.[0-79].post_attention_layernorm,[8192],BF16
model.layers.[0-79].self_attn.k_proj,[1024,8192],BF16
model.layers.[0-79].self_attn.k_proj.bias,[1024],BF16
model.layers.[0-79].self_attn.o_proj,[8192,8192],BF16
model.layers.[0-79].self_attn.q_proj,[8192,8192],BF16
model.layers.[0-79].self_attn.q_proj.bias,[8192],BF16
model.layers.[0-79].self_attn.v_proj,[1024,8192],BF16
model.layers.[0-79].self_attn.v_proj.bias,[1024],BF16
model.norm,[8192],BF16
