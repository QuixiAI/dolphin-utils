meta-llama/Llama-4-Maverick-17B-128E-Instruct

language_model.lm_head.weight,[202048,5120],BF16
language_model.model.embed_tokens.weight,[202048,5120],BF16
language_model.model.layers.[0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 42, 44, 46].feed_forward.down_proj.weight,[5120,16384],BF16
language_model.model.layers.[0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 42, 44, 46].feed_forward.gate_proj.weight,[16384,5120],BF16
language_model.model.layers.[0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 42, 44, 46].feed_forward.up_proj.weight,[16384,5120],BF16
language_model.model.layers.[0-47].input_layernorm.weight,[5120],BF16
language_model.model.layers.[0-47].post_attention_layernorm.weight,[5120],BF16
language_model.model.layers.[0-47].self_attn.k_proj.weight,[1024,5120],BF16
language_model.model.layers.[0-47].self_attn.o_proj.weight,[5120,5120],BF16
language_model.model.layers.[0-47].self_attn.q_proj.weight,[5120,5120],BF16
language_model.model.layers.[0-47].self_attn.v_proj.weight,[1024,5120],BF16
language_model.model.layers.[1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27, 29, 31, 33, 35, 37, 39, 41, 43, 45, 47].feed_forward.experts.down_proj,[128,8192,5120],BF16
language_model.model.layers.[1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27, 29, 31, 33, 35, 37, 39, 41, 43, 45, 47].feed_forward.experts.gate_up_proj,[128,5120,16384],BF16
language_model.model.layers.[1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27, 29, 31, 33, 35, 37, 39, 41, 43, 45, 47].feed_forward.router.weight,[128,5120],BF16
language_model.model.layers.[1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27, 29, 31, 33, 35, 37, 39, 41, 43, 45, 47].feed_forward.shared_expert.down_proj.weight,[5120,8192],BF16
language_model.model.layers.[1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27, 29, 31, 33, 35, 37, 39, 41, 43, 45, 47].feed_forward.shared_expert.gate_proj.weight,[8192,5120],BF16
language_model.model.layers.[1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27, 29, 31, 33, 35, 37, 39, 41, 43, 45, 47].feed_forward.shared_expert.up_proj.weight,[8192,5120],BF16
language_model.model.norm.weight,[5120],BF16
multi_modal_projector.linear_1.weight,[5120,4096],BF16
vision_model.class_embedding,[1408],BF16
vision_model.layernorm_post.bias,[1408],BF16
vision_model.layernorm_post.weight,[1408],BF16
vision_model.layernorm_pre.bias,[1408],BF16
vision_model.layernorm_pre.weight,[1408],BF16
vision_model.model.layers.[0-33].input_layernorm.bias,[1408],BF16
vision_model.model.layers.[0-33].input_layernorm.weight,[1408],BF16
vision_model.model.layers.[0-33].mlp.fc[1-2].bias,[5632],BF16
vision_model.model.layers.[0-33].mlp.fc[1-2].weight,[5632,1408],BF16
vision_model.model.layers.[0-33].post_attention_layernorm.bias,[1408],BF16
vision_model.model.layers.[0-33].post_attention_layernorm.weight,[1408],BF16
vision_model.model.layers.[0-33].self_attn.k_proj.bias,[1408],BF16
vision_model.model.layers.[0-33].self_attn.k_proj.weight,[1408,1408],BF16
vision_model.model.layers.[0-33].self_attn.o_proj.bias,[1408],BF16
vision_model.model.layers.[0-33].self_attn.o_proj.weight,[1408,1408],BF16
vision_model.model.layers.[0-33].self_attn.q_proj.bias,[1408],BF16
vision_model.model.layers.[0-33].self_attn.q_proj.weight,[1408,1408],BF16
vision_model.model.layers.[0-33].self_attn.v_proj.bias,[1408],BF16
vision_model.model.layers.[0-33].self_attn.v_proj.weight,[1408,1408],BF16
vision_model.patch_embedding.linear.weight,[1408,588],BF16
vision_model.positional_embedding_vlm,[577,1408],BF16
vision_model.vision_adapter.mlp.fc[1-2].weight,[4096,5632],BF16
