lm_head.weight,[163840,7168],BF16
model.embed_tokens.weight,[163840,7168],BF16
model.layers.0.mlp.down_proj.weight,[7168,18432],BF16
model.layers.0.mlp.gate_proj.weight,[18432,7168],BF16
model.layers.0.mlp.up_proj.weight,[18432,7168],BF16
model.layers.[0-60].input_layernorm.weight,[7168],BF16
model.layers.[0-60].post_attention_layernorm.weight,[7168],BF16
model.layers.[0-60].self_attn.kv_a_layernorm.weight,[512],BF16
model.layers.[0-60].self_attn.kv_a_proj_with_mqa.weight,[576,7168],BF16
model.layers.[0-60].self_attn.kv_b_proj.weight,[16384,512],BF16
model.layers.[0-60].self_attn.o_proj.weight,[7168,8192],BF16
model.layers.[0-60].self_attn.q_a_layernorm.weight,[1536],BF16
model.layers.[0-60].self_attn.q_a_proj.weight,[1536,7168],BF16
model.layers.[0-60].self_attn.q_b_proj.weight,[12288,1536],BF16
model.layers.[0-60].self_attn.rotary_emb.inv_freq,[56],BF16
model.layers.[1-60].mlp.experts.[0-383].down_proj.weight,[7168,2048],BF16
model.layers.[1-60].mlp.experts.[0-383].gate_proj.weight,[2048,7168],BF16
model.layers.[1-60].mlp.experts.[0-383].up_proj.weight,[2048,7168],BF16
model.layers.[1-60].mlp.gate.e_score_correction_bias,[384],F32
model.layers.[1-60].mlp.gate.weight,[384,7168],BF16
model.layers.[1-60].mlp.shared_experts.down_proj.weight,[7168,2048],BF16
model.layers.[1-60].mlp.shared_experts.gate_proj.weight,[2048,7168],BF16
model.layers.[1-60].mlp.shared_experts.up_proj.weight,[2048,7168],BF16
model.norm.weight,[7168],BF16