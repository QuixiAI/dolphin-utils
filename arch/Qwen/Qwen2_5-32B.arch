lm_head,[152064,5120],BF16
model.embed_tokens,[152064,5120],BF16%
model.layers.[0-63].input_layernorm,[5120],BF16
model.layers.[0-63].mlp.down_proj,[5120,27648],BF16
model.layers.[0-63].mlp.gate_proj,[27648,5120],BF16
model.layers.[0-63].mlp.up_proj,[27648,5120],BF16
model.layers.[0-63].post_attention_layernorm,[5120],BF16
model.layers.[0-63].self_attn.k_proj,[1024,5120],BF16
model.layers.[0-63].self_attn.k_proj.bias,[1024],BF16
model.layers.[0-63].self_attn.o_proj,[5120,5120],BF16
model.layers.[0-63].self_attn.q_proj,[5120,5120],BF16
model.layers.[0-63].self_attn.q_proj.bias,[5120],BF16
model.layers.[0-63].self_attn.v_proj,[1024,5120],BF16
model.layers.[0-63].self_attn.v_proj.bias,[1024],BF16
model.norm,[5120],BF16