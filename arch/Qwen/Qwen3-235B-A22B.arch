lm_head,[151936,4096],BF16
model.embed_tokens,[151936,4096],BF16
model.layers.[0-93].input_layernorm,[4096],BF16
model.layers.[0-93].mlp.experts.[0-127].down_proj,[4096,1536],BF16
model.layers.[0-93].mlp.experts.[0-127].gate_proj,[1536,4096],BF16
model.layers.[0-93].mlp.experts.[0-127].up_proj,[1536,4096],BF16
model.layers.[0-93].mlp.gate,[128,4096],BF16
model.layers.[0-93].post_attention_layernorm,[4096],BF16
model.layers.[0-93].self_attn.k_norm,[128],BF16
model.layers.[0-93].self_attn.k_proj,[512,4096],BF16
model.layers.[0-93].self_attn.o_proj,[4096,8192],BF16
model.layers.[0-93].self_attn.q_norm,[128],BF16
model.layers.[0-93].self_attn.q_proj,[8192,4096],BF16
model.layers.[0-93].self_attn.v_proj,[512,4096],BF16
model.norm,[4096],BF16
