lm_head,[151936,4096],BF16
model.embed_tokens,[151936,4096],BF16
model.layers.[0-35].input_layernorm,[4096],BF16
model.layers.[0-35].mlp.down_proj,[4096,12288],BF16
model.layers.[0-35].mlp.gate_proj,[12288,4096],BF16
model.layers.[0-35].mlp.up_proj,[12288,4096],BF16
model.layers.[0-35].post_attention_layernorm,[4096],BF16
model.layers.[0-35].self_attn.k_norm,[128],BF16
model.layers.[0-35].self_attn.k_proj,[1024,4096],BF16
model.layers.[0-35].self_attn.o_proj,[4096,4096],BF16
model.layers.[0-35].self_attn.q_norm,[128],BF16
model.layers.[0-35].self_attn.q_proj,[4096,4096],BF16
model.layers.[0-35].self_attn.v_proj,[1024,4096],BF16
model.norm,[4096],BF16
