lm_head,[151936,1024],BF16
model.embed_tokens,[151936,1024],BF16
model.layers.[0-27].input_layernorm,[1024],BF16
model.layers.[0-27].mlp.down_proj,[1024,3072],BF16
model.layers.[0-27].mlp.gate_proj,[3072,1024],BF16
model.layers.[0-27].mlp.up_proj,[3072,1024],BF16
model.layers.[0-27].post_attention_layernorm,[1024],BF16
model.layers.[0-27].self_attn.k_norm,[128],BF16
model.layers.[0-27].self_attn.k_proj,[1024,1024],BF16
model.layers.[0-27].self_attn.o_proj,[1024,2048],BF16
model.layers.[0-27].self_attn.q_norm,[128],BF16
model.layers.[0-27].self_attn.q_proj,[2048,1024],BF16
model.layers.[0-27].self_attn.v_proj,[1024,1024],BF16
model.norm,[1024],BF16
