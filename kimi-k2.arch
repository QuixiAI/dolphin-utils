lm_head.weight,[163840,7168],BF16
model.embed_tokens.weight,[163840,7168],BF16
model.layers.0.mlp.down_proj.weight,[7168,18432],F8_E4M3
model.layers.0.mlp.down_proj.weight_scale_inv,[56,144],F32
model.layers.0.mlp.gate_proj.weight,[18432,7168],F8_E4M3
model.layers.0.mlp.gate_proj.weight_scale_inv,[144,56],F32
model.layers.0.mlp.up_proj.weight,[18432,7168],F8_E4M3
model.layers.0.mlp.up_proj.weight_scale_inv,[144,56],F32
model.layers.[0-60].input_layernorm.weight,[7168],BF16
model.layers.[0-60].post_attention_layernorm.weight,[7168],BF16
model.layers.[0-60].self_attn.kv_a_layernorm.weight,[512],BF16
model.layers.[0-60].self_attn.kv_a_proj_with_mqa.weight,[576,7168],F8_E4M3
model.layers.[0-60].self_attn.kv_a_proj_with_mqa.weight_scale_inv,[5,56],F32
model.layers.[0-60].self_attn.kv_b_proj.weight,[16384,512],F8_E4M3
model.layers.[0-60].self_attn.kv_b_proj.weight_scale_inv,[128,4],F32
model.layers.[0-60].self_attn.o_proj.weight,[7168,8192],F8_E4M3
model.layers.[0-60].self_attn.o_proj.weight_scale_inv,[56,64],F32
model.layers.[0-60].self_attn.q_a_layernorm.weight,[1536],BF16
model.layers.[0-60].self_attn.q_a_proj.weight,[1536,7168],F8_E4M3
model.layers.[0-60].self_attn.q_a_proj.weight_scale_inv,[12,56],F32
model.layers.[0-60].self_attn.q_b_proj.weight,[12288,1536],F8_E4M3
model.layers.[0-60].self_attn.q_b_proj.weight_scale_inv,[96,12],F32
model.layers.[0-60].self_attn.rotary_emb.inv_freq,[56],BF16
model.layers.[1-60].mlp.experts.[0-383].down_proj.weight,[7168,2048],F8_E4M3
model.layers.[1-60].mlp.experts.[0-383].down_proj.weight_scale_inv,[56,16],F32
model.layers.[1-60].mlp.experts.[0-383].gate_proj.weight,[2048,7168],F8_E4M3
model.layers.[1-60].mlp.experts.[0-383].gate_proj.weight_scale_inv,[16,56],F32
model.layers.[1-60].mlp.experts.[0-383].up_proj.weight,[2048,7168],F8_E4M3
model.layers.[1-60].mlp.experts.[0-383].up_proj.weight_scale_inv,[16,56],F32
model.layers.[1-60].mlp.gate.e_score_correction_bias,[384],F32
model.layers.[1-60].mlp.gate.weight,[384,7168],BF16
model.layers.[1-60].mlp.shared_experts.down_proj.weight,[7168,2048],F8_E4M3
model.layers.[1-60].mlp.shared_experts.down_proj.weight_scale_inv,[56,16],F32
model.layers.[1-60].mlp.shared_experts.gate_proj.weight,[2048,7168],F8_E4M3
model.layers.[1-60].mlp.shared_experts.gate_proj.weight_scale_inv,[16,56],F32
model.layers.[1-60].mlp.shared_experts.up_proj.weight,[2048,7168],F8_E4M3
model.layers.[1-60].mlp.shared_experts.up_proj.weight_scale_inv,[16,56],F32
model.norm.weight,[7168],BF16


# QuixiAI/Kimi-K2-Instruct-BF16
lm_head.weight,[163840,7168],BF16
model.embed_tokens.weight,[163840,7168],BF16
model.layers.0.mlp.down_proj.weight,[7168,18432],BF16
model.layers.0.mlp.gate_proj.weight,[18432,7168],BF16
model.layers.0.mlp.up_proj.weight,[18432,7168],BF16
model.layers.[0-60].input_layernorm.weight,[7168],BF16
model.layers.[0-60].post_attention_layernorm.weight,[7168],BF16
model.layers.[0-60].self_attn.kv_a_layernorm.weight,[512],BF16
model.layers.[0-60].self_attn.kv_a_proj_with_mqa.weight,[576,7168],BF16
model.layers.[0-60].self_attn.kv_b_proj.weight,[16384,512],BF16
model.layers.[0-60].self_attn.o_proj.weight,[7168,8192],BF16
model.layers.[0-60].self_attn.q_a_layernorm.weight,[1536],BF16
model.layers.[0-60].self_attn.q_a_proj.weight,[1536,7168],BF16
model.layers.[0-60].self_attn.q_b_proj.weight,[12288,1536],BF16
model.layers.[0-60].self_attn.rotary_emb.inv_freq,[56],BF16
model.layers.[1-60].mlp.experts.[0-383].down_proj.weight,[7168,2048],BF16
model.layers.[1-60].mlp.experts.[0-383].gate_proj.weight,[2048,7168],BF16
model.layers.[1-60].mlp.experts.[0-383].up_proj.weight,[2048,7168],BF16
model.layers.[1-60].mlp.gate.e_score_correction_bias,[384],F32
model.layers.[1-60].mlp.gate.weight,[384,7168],BF16
model.layers.[1-60].mlp.shared_experts.down_proj.weight,[7168,2048],BF16
model.layers.[1-60].mlp.shared_experts.gate_proj.weight,[2048,7168],BF16
model.layers.[1-60].mlp.shared_experts.up_proj.weight,[2048,7168],BF16
model.norm.weight,[7168],BF16
